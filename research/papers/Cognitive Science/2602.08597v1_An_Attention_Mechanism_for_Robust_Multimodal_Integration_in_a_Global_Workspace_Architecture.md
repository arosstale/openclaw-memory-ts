# An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture

**arXiv ID:** 2602.08597v1
**Domain:** Cognitive Science
**Published:** 2026-02-09
**Updated:** 2026-02-09
**Primary Category:** cs.AI

## Authors
Roland Bertin-Johannet, Lara Scipio, Leopold Mayti√©, Rufin VanRullen

## Categories
cs.AI

## Abstract
Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

## PDF
[Download PDF](https://arxiv.org/pdf/2602.08597v1)

## Source
[arXiv Page](2602.08597v1)

---
*Fetched: 2026-02-15T14:43:06.439497*
*Tags: #AI*
