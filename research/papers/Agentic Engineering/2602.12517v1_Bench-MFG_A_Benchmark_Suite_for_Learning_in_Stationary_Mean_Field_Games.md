# Bench-MFG: A Benchmark Suite for Learning in Stationary Mean Field Games

**arXiv ID:** 2602.12517v1
**Domain:** Agentic Engineering
**Published:** 2026-02-13
**Updated:** 2026-02-13
**Primary Category:** cs.LG

## Authors
Lorenzo Magnino, Jiacheng Shen, Matthieu Geist, Olivier Pietquin, Mathieu Lauri√®re

## Categories
cs.LG, cs.AI, cs.MA

## Abstract
The intersection of Mean Field Games (MFGs) and Reinforcement Learning (RL) has fostered a growing family of algorithms designed to solve large-scale multi-agent systems. However, the field currently lacks a standardized evaluation protocol, forcing researchers to rely on bespoke, isolated, and often simplistic environments. This fragmentation makes it difficult to assess the robustness, generalization, and failure modes of emerging methods. To address this gap, we propose a comprehensive benchmark suite for MFGs (Bench-MFG), focusing on the discrete-time, discrete-space, stationary setting for the sake of clarity. We introduce a taxonomy of problem classes, ranging from no-interaction and monotone games to potential and dynamics-coupled games, and provide prototypical environments for each. Furthermore, we propose MF-Garnets, a method for generating random MFG instances to facilitate rigorous statistical testing. We benchmark a variety of learning algorithms across these environments, including a novel black-box approach (MF-PSO) for exploitability minimization. Based on our extensive empirical results, we propose guidelines to standardize future experimental comparisons. Code available at \href{https://github.com/lorenzomagnino/Bench-MFG}{https://github.com/lorenzomagnino/Bench-MFG}.

## PDF
[Download PDF](https://arxiv.org/pdf/2602.12517v1)

## Source
[arXiv Page](2602.12517v1)

---
*Fetched: 2026-02-16T09:00:35.376299*
*Tags: #LG #AI #MA*
