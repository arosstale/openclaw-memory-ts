# Time-Inhomogeneous Volatility Aversion for Financial Applications of Reinforcement Learning

**arXiv ID:** 2602.12030v1
**Domain:** Trading & Finance
**Published:** 2026-02-12
**Updated:** 2026-02-12
**Primary Category:** q-fin.CP

## Authors
Federico Cacciamani, Roberto Daluiso, Marco Pinciroli, Michele Trapletti, Edoardo Vittori

## Categories
q-fin.CP, q-fin.TR

## Abstract
In finance, sequential decision problems are often faced, for which reinforcement learning (RL) emerges as a promising tool for optimisation without the need of analytical tractability. However, the objective of classical RL is the expected cumulated reward, while financial applications typically require a trade-off between return and risk. In this work, we focus on settings where one cares about the time split of the total return, ruling out most risk-aware generalisations of RL which optimise a risk measure defined on the latter. We notice that a preference for homogeneous splits, which we found satisfactory for hedging, can be unfit for other problems, and therefore propose a new risk metric which still penalises uncertainty of the single rewards, but allows for an arbitrary planning of their target levels. We study the properties of the resulting objective and the generalisation of learning algorithms to optimise it. Finally, we show numerical results on toy examples.

## PDF
[Download PDF](https://arxiv.org/pdf/2602.12030v1)

## Source
[arXiv Page](2602.12030v1)

---
*Fetched: 2026-02-15T14:43:03.789164*
*Tags: #q-fin.CP #q-fin.TR*
